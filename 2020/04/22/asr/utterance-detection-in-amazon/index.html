
<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
  
  
    <meta name="keywords" content="ASR,论文,amazon,Utterance Detection," />
  

  
    <meta name="description" content="Stay hungry, Stay foolish" />
  
  
  
  <link rel="icon" type="image/x-icon" href="/images/logo-shan.jpg">
  
  <title>Utterance Detection in Amazon [ 羊角岽 ]</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css">
    
      <link rel="stylesheet" href="/css/less.css">
    
      <link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
    
  
<meta name="generator" content="Hexo 4.2.0"><link rel="stylesheet" href="/css/prism-vs.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>


<body>
  <div class="nav-container">
    <nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    
      <img class="avatar" src="http://blog.kingway.fun/images/icon-shan.png">
    
    <span class="title" style="text-transform:none">羊角岽</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            
              <li class="pure-menu-item"><a href="/" class="pure-menu-link">首页</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/categories" class="pure-menu-link">分类</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/tags" class="pure-menu-link">标签</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">归档</a></li>
            
          
      
          
            
              <li class="pure-menu-item"><a href="/about" class="pure-menu-link">关于</a></li>
            
          
      
  </ul>
   
</nav>

  </div>

  <div class="container" id="content-outer">
    <div class="inner" id="content-inner">
      <div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        Utterance Detection in Amazon
      </h1>
      <span>
        
        <time class="time" datetime="2020-04-21T22:34:44.000Z">
        2020-04-22
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ASR/" rel="tag">ASR</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Utterance-Detection/" rel="tag">Utterance Detection</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/amazon/" rel="tag">amazon</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a></li></ul>
      </span>
    </span>
      <span class="slash">/</span>
      <span class="read">
      <span id="busuanzi_value_page_pv"></span> 点击
    </span>
    </header>

    <div class="post-content">
      <p>最近粗略的接触了些话音检测和噪声过滤的工作，有参考到Amazon的相关论文，在此分享下论文的主要方法和贡献。本次分享的两篇论文方法套路基本一样，只是分别使用于不同的场景，一篇是用在端点检测（END-OF-UTTERANCE DETECTION）上，用来判断用户是否已经结束语音指令。另一篇是应用在Device-directed Utterance Detection上，应该主要是用在echo音箱等vpa上，用来过滤一些误触发，用户正对着设备下发的指令认为是正例，而背景噪音，细碎的嬉笑打闹聊天的声音认为是负例，不应该触发， 所以叫Device-directed Utterance Detection。</p>
<h2 id="Device-directed-Utterance-Detection"><a href="#Device-directed-Utterance-Detection" class="headerlink" title="Device-directed Utterance Detection"></a>Device-directed Utterance Detection</h2><h3 id="方法及贡献"><a href="#方法及贡献" class="headerlink" title="方法及贡献"></a>方法及贡献</h3><p>模型可简单概括为三种特征+一个分类器，作者用两个LSTM分别对声学和识别结果分别做embedding，再加上decoder输出的一些特征，最后将三个特征结合起来作为最后分类器的输入。系统结构如下右图所示。这种做法还是比较直观的，三部分特征刚好对应语音识别系统三大部件的输出，对 1-best hypotheses的处理基本可以理解为对语言模型的embedding。最后是一个二分类器，判断本次query是否是Device-directed话音。下左图是正负例的一些样例。</p>
<table>
<thead>
<tr>
<th><!-- --></th>
<th><!-- --></th>
</tr>
</thead>
<tbody><tr>
<td><img src="http://img.kingway.fun/IMGMatrix/blog/2020/04/amazon-spd-2.png" alt=""></td>
<td><img src="http://img.kingway.fun/IMGMatrix/blog/2020/04/amazon-spd-1.png" alt=""></td>
</tr>
</tbody></table>
<h3 id="模型与参数"><a href="#模型与参数" class="headerlink" title="模型与参数"></a>模型与参数</h3><h4 id="Acoustic-Embedding"><a href="#Acoustic-Embedding" class="headerlink" title="Acoustic Embedding"></a>Acoustic Embedding</h4><p>Acoustic Embedding 用LSTM做帧级别的device-directed target的预测，输入是每一帧log filter-bank energies (LFBEs)，以25ms帧长和10ms帧移分帧，每一帧的label都和整条query的label一样，最后使用cross-entropy Loss，SGD训练。做softmax之前的两维向量拿出来，作为Acoustic Embedding。</p>
<h4 id="ASR-feature"><a href="#ASR-feature" class="headerlink" title="ASR feature"></a>ASR feature</h4><p>ASR的输出特征论文里讲的有18维，但未全部给出，只介绍里其中的三维，其一是帧前向概率的平均值，先计算每一帧的前向概率的熵，所有帧的均值即为所得，熵越大说明前向的概率分布差不多，1-best的confidence就越低。 其二是1-best的viterbi cost。 其三是计算lattice节点出边的平均值。</p>
<h4 id="Character-Embedding"><a href="#Character-Embedding" class="headerlink" title="Character Embedding"></a>Character Embedding</h4><p>Character Embedding和Acoustic Embedding的思路基本一样，也是使用LSTM。只不过输入是1-best输出character级别的embedding，每个character的embedding <a href="https://github.com/stanfordnlp/GloVe" target="_blank" rel="noopener">GloVE</a>来训练。也是取最后一个character的输出作为这部分的Embedding（2维）。</p>
<h4 id="Classification-Layer"><a href="#Classification-Layer" class="headerlink" title="Classification Layer"></a>Classification Layer</h4><p>最后一个分类层使用的就是简单的两层前馈神经网络，在另一篇论文里有讲，隐藏层节点为100。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>论文的实验数据是在他们自己产品的数据集，并未公开，下面是一些实验结果。</p>
<ul>
<li><p>参数选择<br>作者对LSTM的层数和节点数以及character embedding的维数做了对比实验，实验结果如下图。</p>
<table>
<thead>
<tr>
<th><!-- --></th>
<th><!-- --></th>
</tr>
</thead>
<tbody><tr>
<td><img src="http://img.kingway.fun/IMGMatrix/blog/2020/04/amazon-spd-3.png" alt=""></td>
<td><img src="http://img.kingway.fun/IMGMatrix/blog/2020/04/amazon-spd-4.png" alt=""></td>
</tr>
</tbody></table>
</li>
<li><p>性能比较<br>作者还对比了三种单独特征及融合特征对分类的影响，实验显示单单ASR decoder 特征效果最好，多特征的话效果更加。模型用Detection error tradeoff (DET)评价，跟ROC曲线差不多的意思，只是横纵轴定义不一样。</p>
<table>
<thead>
<tr>
<th><!-- --></th>
<th><!-- --></th>
</tr>
</thead>
<tbody><tr>
<td><img src="http://img.kingway.fun/IMGMatrix/blog/2020/04/amazon-spd-5.png" alt=""></td>
<td><img src="http://img.kingway.fun/IMGMatrix/blog/2020/04/amazon-spd-6.png" alt=""></td>
</tr>
</tbody></table>
</li>
</ul>
<p>我们参考了此篇论文的思路，目前只使用了ASR Decoder的特征，不过由于decoder实现不一样，采用的特征和维度都不一样，效果也还行，这是可以解释的，ASR Decoder输出的特征是声学模型和语言模型这两个大模型生成的，区分度应该不错。</p>
<h2 id="End-of-Utterance-detection"><a href="#End-of-Utterance-detection" class="headerlink" title="End-of-Utterance detection"></a>End-of-Utterance detection</h2><p>End-of-Utterance检测也叫enpoint检测，主要是判断用户是否说完话了，对端到端的速度和用户体验至关重要，既不能提前（会打断用户说话），也不能延后（端到端速度慢，体验差）。简单说endpoint检测的目标就是保持准确率高的前提下尽可能快的检测到用户说完话的意图，这当然需要trade-off，一个灵敏的检测器有可能会截断用户的query导致准确率下降。 <a href="https://github.com/kaldi-asr/kaldi" target="_blank" rel="noopener">Kaldi</a>里面的通常做法是计算解码时lattice里面的末尾silence帧的数量，达到一定长度并且满足对应的cost阈值就认为是endpoint，这种方法纯从解码器的角度出发，利用声学模型的输出，没有对声学特征做另外的建模。工业实践上为了效果和计算资源，一般还会在语音识别前面加一个voice-activity-detection (VAD)检测，本文也是这个思路。 这篇论文发表的时间要早于上面那篇，而且要解决的问题也比上篇要复杂。</p>
<h3 id="方法及贡献-1"><a href="#方法及贡献-1" class="headerlink" title="方法及贡献"></a>方法及贡献</h3><p>本文模型结构基本和上文是一样的（上文借鉴本文）， 特征也是三个部分组成Acoustic Eembedding和 1-best hypotheses Eembedding和ASR输出的feature，最后加个二分类器，不过上文主要论证比较ASR feature的有效性，此本则偏重与Acoustic部分。另外，这篇论文是在远场环境上做的，一般来说远场环境的困难要大得多。</p>
<table>
<thead>
<tr>
<th><!-- --></th>
<th><!-- --></th>
</tr>
</thead>
<tbody><tr>
<td><img src="http://img.kingway.fun/IMGMatrix/blog/2020/04/amazon-epd-2.png" alt=""></td>
<td><img src="http://img.kingway.fun/IMGMatrix/blog/2020/04/amazon-epd-1.png" alt=""></td>
</tr>
</tbody></table>
<p>不同的地方主要有以下几点：</p>
<ul>
<li>Acoustic 部分做的是多任务学习，将声学feature同时用于训练VAD和endpoint检测，而且与上文不同，label是帧级别的，所以训练时需要对齐特征和label，如上右图所示。取VAD部分取输出给Inference时计算阈值，endpoint部分则取pre-softmax的特征作为最后二分类器的输入。</li>
<li>1-best hypotheses embedding部分使用的是word embedding，由于输出和帧无法一一对应，如果ASR输出的partial没有变化，则会一直延用最新的word embedding。</li>
<li>ASR feature 方面只介绍了个pause duration，这个应该跟kaidi检测endpoint类似，计算lattice（或其他类似解码网格）的末尾静音帧数。</li>
</ul>
<h3 id="Inference-Pipeline"><a href="#Inference-Pipeline" class="headerlink" title="Inference Pipeline"></a>Inference Pipeline</h3><p>上面介绍Acoustic部分分为两部分vad + endpoint，在inference是先用vad的输出判断静音的持续时间，通过阈值Tmin，Tmax来判断是否启用endpoint检测（二分类器的输出）。在下面两种情况下认为检测到endpoint，当vad给出的静音阈值大于Tmin并且最终二分类器输出为endpoint，或者vad给出的静音阈值超过Tmax。 endpoint检测是一个比较困难的题目，实际工业应用基本不可能单模型就解决得很好，一般都使用多模型融合。</p>
<h3 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h3><ul>
<li><p>特征性能比较<br>实验部分照例比较了三种特征的性能和融合后的性能，Acoustic 单特征最后，融合能带来性能的提升。</p>
<table>
<thead>
<tr>
<th><!-- --></th>
<th><!-- --></th>
</tr>
</thead>
<tbody><tr>
<td><img src="http://img.kingway.fun/IMGMatrix/blog/2020/04/amazon-epd-3.png" alt=""></td>
<td><img src="http://img.kingway.fun/IMGMatrix/blog/2020/04/amazon-epd-4.png" alt=""></td>
</tr>
</tbody></table>
</li>
<li><p>混合语言的效果<br>作者还进行了混合语言的效果，说明本模型跟语言并不太相关，即模型架构不需要改变，换批训练数据，在其他语言上也能达到相同的效果。这当然是最理想的结果，相关实验结果如下。</p>
</li>
</ul>
<table>
<thead>
<tr>
<th><!-- --></th>
</tr>
</thead>
<tbody><tr>
<td><img src="http://img.kingway.fun/IMGMatrix/blog/2020/04/amazon-epd-5.png" alt=""></td>
</tr>
</tbody></table>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li>[1] <a href="https://docs.qq.com/pdf/DRUtsWlVCZ2lFZERU" target="_blank" rel="noopener">Device-directed Utterance Detection</a></li>
<li>[2] <a href="">COMBINING ACOUSTIC EMBEDDINGSAND DECODING FEATURES FOR END-OF-UTTERANCE DETECTION IN REAL-TIME FAR-FIELD SPEECH RECOGNITION SYSTEMS</a></li>
</ul>

    </div>

  </article>
  <div class="toc-container">
    
  <div id="toc" class="toc-article">
    <strong class="toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Device-directed-Utterance-Detection"><span class="toc-text">Device-directed Utterance Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#方法及贡献"><span class="toc-text">方法及贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型与参数"><span class="toc-text">模型与参数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Acoustic-Embedding"><span class="toc-text">Acoustic Embedding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ASR-feature"><span class="toc-text">ASR feature</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Character-Embedding"><span class="toc-text">Character Embedding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Classification-Layer"><span class="toc-text">Classification Layer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验结果"><span class="toc-text">实验结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#End-of-Utterance-detection"><span class="toc-text">End-of-Utterance detection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#方法及贡献-1"><span class="toc-text">方法及贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Inference-Pipeline"><span class="toc-text">Inference Pipeline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验结果-1"><span class="toc-text">实验结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参考文献"><span class="toc-text">参考文献</span></a></li></ol></li></ol>
  </div>


  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener">知识共享署名 4.0 国际许可协议</a>
    <span>进行许可。 转载时请注明原文链接。</span>
</div>


  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/2020/03/28/asr/lihongyi-nlp/" rel="next" title="深度学习人类语言处理 公开课">
          深度学习人类语言处理 公开课
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
      </div>
    </div>
  

    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
    <div id="gitalk-container"></div>
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script type="text/javascript">
        var gitalk = new Gitalk({
            clientID: '008534a3472177954e48',
            clientSecret: '165d863aaa34f99f350f99a663f410c0b49a4dce',
            id: window.location.pathname,
            repo: 'pkufool.github.io',
            owner: 'pkufool',
            admin: 'pkufool'
        })
        gitalk.render('gitalk-container')
    </script>



    </div>

    

  </div>
  <footer class="footer text-center">
    <div id="bottom-inner">
        
        <a class="bottom-item" href="/">© 2017-2020 羊角岽</a> |
        <a class="bottom-item" href="http://www.beian.miit.gov.cn" target="_blank">京ICP备19047165号-1</a> |
        <a class="bottom-item" href="https://hexo.io" target="_blank">Powered by hexo</a> |
        <a class="bottom-item" href="" target="_blank">Theme less</a>
    </div>
</footer>

  

<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>

  



  
<script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
</script>


</body>
</html>
